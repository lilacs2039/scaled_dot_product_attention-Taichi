{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64734be2-7609-46bb-bdab-055f965100c6",
   "metadata": {},
   "source": [
    "# scaled_dot_product_attention-Taichi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c953e7-275c-4de3-a706-59f36698d928",
   "metadata": {},
   "source": [
    "## Taichi版 scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ea537-8fe4-4f6d-85a7-142fd6574e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] version 1.7.0, llvm 15.0.4, commit 2fd24490, linux, python 3.11.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 03/20/24 22:09:31.365 193278] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n",
      "[W 03/20/24 22:09:31.456 193278] [cuda_driver.cpp:load_lib@36] libcuda.so lib not found.\n",
      "RHI Error: GLFW Error 65543: GLX: Failed to create context: GLXBadFBConfig\n",
      "[W 03/20/24 22:09:31.823 193278] [opengl_api.cpp:initialize_opengl@205] Can not create OpenGL context\n",
      "[W 03/20/24 22:09:31.825 193278] [misc.py:adaptive_arch_select@758] Arch=[<Arch.cuda: 3>, <Arch.metal: 4>, <Arch.vulkan: 10>, <Arch.opengl: 5>, <Arch.dx11: 6>, <Arch.dx12: 7>, <Arch.gles: 11>, <Arch.amdgpu: 9>] is not supported, falling back to CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=x64\n",
      "out:  [[[[0.87 0.83 0.24 0.32 0.93 0.92 0.91 0.76]\n",
      "   [0.65 0.90 0.14 0.28 0.63 0.56 0.53 0.57]\n",
      "   [0.62 0.61 0.26 0.44 0.58 0.55 0.49 0.64]\n",
      "   [0.63 0.64 0.19 0.37 0.46 0.54 0.50 0.64]]\n",
      "\n",
      "  [[0.07 0.89 0.75 0.24 0.70 0.61 0.91 0.82]\n",
      "   [0.20 0.74 0.67 0.33 0.74 0.67 0.73 0.79]\n",
      "   [0.37 0.68 0.74 0.53 0.61 0.64 0.79 0.77]\n",
      "   [0.50 0.61 0.58 0.54 0.55 0.50 0.76 0.61]]]]\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(formatter={'float': '{:.2f}'.format})\n",
    "\n",
    "# IS_DEBUG = True\n",
    "IS_DEBUG = False\n",
    "BACKEND = ti.gpu\n",
    "ti.init(BACKEND, debug=IS_DEBUG)\n",
    "\n",
    "# パラメータ設定\n",
    "batch_size, head_size, sequence_size, embedding_size = 1, 2, 4, 8\n",
    "\n",
    "# Taichi fields\n",
    "Q = ti.field(dtype=ti.f32, shape=(batch_size, head_size, sequence_size, embedding_size))\n",
    "K = ti.field(dtype=ti.f32, shape=(batch_size, head_size, sequence_size, embedding_size))\n",
    "V = ti.field(dtype=ti.f32, shape=(batch_size, head_size, sequence_size, embedding_size))\n",
    "out = ti.field(dtype=ti.f32, shape=(batch_size, head_size, sequence_size, embedding_size))\n",
    "\n",
    "@ti.kernel\n",
    "def init_attention(q:ti.template(), k:ti.template(), v:ti.template(), out:ti.template()):\n",
    "    # ダミーデータの生成\n",
    "    for I in ti.grouped(q):\n",
    "        Q[I] = ti.random()\n",
    "        K[I] = ti.random()\n",
    "        V[I] = ti.random()\n",
    "    out.fill(0)  # 出力の初期化    \n",
    "init_attention(Q, K, V, out)\n",
    "\n",
    "@ti.func\n",
    "def max2d(matrix:ti.template()) -> ti.f32:\n",
    "    \"テンソル全体の最大値を計算\"\n",
    "    max_val = 1e-10\n",
    "    for i,j in ti.ndrange(matrix.n, matrix.m):\n",
    "        max_val = ti.max(max_val, matrix[i,j])\n",
    "    return max_val\n",
    "\n",
    "@ti.func\n",
    "def softmax2d(mat:ti.template()):\n",
    "    \"softmaxを計算して、引数の行列を書き換える。\"\n",
    "    n,m = mat.n, mat.m\n",
    "    mat_max = max2d(mat)\n",
    "    for s,_s in ti.ndrange(n,m):\n",
    "        mat[s, _s] = ti.exp(mat[s, _s] - mat_max)  # 指数関数を計算する前に最大値を引く\n",
    "    for s in range(n):\n",
    "        sum_exp = 0.0\n",
    "        for _s in range(m):\n",
    "            sum_exp += mat[s, _s]  # _s についての和を計算\n",
    "        for _s in range(m):\n",
    "            mat[s, _s] = mat[s, _s] / sum_exp  # _s についての和で割る（正規化）\n",
    "                \n",
    "\n",
    "@ti.kernel\n",
    "def scaled_dotproduct_attention(q: ti.template(), k: ti.template(), v: ti.template(), out: ti.template()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---------------------\n",
    "    q,k,v,out : tensor with shape [... s e]\n",
    "          s : sequence\n",
    "          e : embedding\n",
    "    \"\"\"\n",
    "    assert q.shape == k.shape == v.shape == out.shape\n",
    "    # 形状からsequence_sizeとembedding_sizeを取得\n",
    "    upper_dims = ti.static(q.shape[:-2])  # 最初の2次元\n",
    "    sequence_size, embedding_size = ti.static(q.shape[-2:])  # 最後の2次元\n",
    "\n",
    "    for I in ti.static(ti.ndrange(*upper_dims)):\n",
    "        mat = ti.Matrix([[-1e9] * sequence_size for _ in ti.ndrange(sequence_size)], ti.f32)\n",
    "        \n",
    "        # attention scoreを計算\n",
    "        for s, _s in ti.ndrange(sequence_size, sequence_size):\n",
    "            if s < _s: continue  # Causal mask : s <= _s だけを計算\n",
    "            mat[s, _s] = 0.0\n",
    "            for e in range(embedding_size): mat[s, _s] += q[I, s, e] * k[I, _s, e]\n",
    "            mat[s, _s] *= (1.0 / ti.sqrt(embedding_size))\n",
    "\n",
    "        if IS_DEBUG: print(\"att: \", I, mat)\n",
    "\n",
    "        # Softmax\n",
    "        softmax2d(mat)\n",
    "\n",
    "        if IS_DEBUG: print(\"softmax(att): \", I, mat)\n",
    "\n",
    "        # 出力の計算\n",
    "        for s, _s, e in ti.ndrange(sequence_size, sequence_size, embedding_size):\n",
    "            out[I, s, e] += mat[s, _s] * v[I, _s, e]\n",
    "\n",
    "scaled_dotproduct_attention(Q, K, V, out)\n",
    "ti.sync()\n",
    "time.sleep(0.1)\n",
    "print(\"out: \", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68db123-d184-406b-80d6-0ee9002178bf",
   "metadata": {},
   "source": [
    "### torch版　scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aeb8b-5940-482a-9f72-d0922de7d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  [[[[0.87 0.83 0.24 0.32 0.93 0.92 0.91 0.76]\n",
      "   [0.65 0.90 0.14 0.28 0.63 0.56 0.53 0.57]\n",
      "   [0.62 0.61 0.26 0.44 0.58 0.55 0.49 0.64]\n",
      "   [0.63 0.64 0.19 0.37 0.46 0.54 0.50 0.64]]\n",
      "\n",
      "  [[0.07 0.89 0.75 0.24 0.70 0.61 0.91 0.82]\n",
      "   [0.20 0.74 0.67 0.33 0.74 0.67 0.73 0.79]\n",
      "   [0.37 0.68 0.74 0.53 0.61 0.64 0.79 0.77]\n",
      "   [0.50 0.61 0.58 0.54 0.55 0.50 0.76 0.61]]]]\n",
      "diff:  [[[[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]\n",
      "\n",
      "  [[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "   [0.00 0.00 0.00 0.00 -0.00 -0.00 -0.00 -0.00]]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "out_torch = torch.nn.functional.scaled_dot_product_attention(Q.to_torch(), K.to_torch(), V.to_torch(), attn_mask=None, dropout_p=0\n",
    "                                                            # , is_causal=False\n",
    "                                                            , is_causal = True\n",
    "                                                           )\n",
    "print(\"out: \", out_torch.numpy())\n",
    "print(\"diff: \", out.to_numpy()-out_torch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4790547-0b8b-4c0f-b2da-b7dec087d5bc",
   "metadata": {},
   "source": [
    "### Python版scaled_dot_product_attention\n",
    "\n",
    "参考　https://github.com/karpathy/nanoGPT/blob/master/model.py#L66\n",
    "\n",
    "※Causal maskなしで実行。そのためoutの値は異なる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55340b4-ace4-44c5-9f32-b1917127bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  [[[[0.64 0.64 0.19 0.36 0.45 0.55 0.51 0.65]\n",
      "   [0.65 0.64 0.20 0.37 0.48 0.56 0.52 0.66]\n",
      "   [0.65 0.63 0.20 0.37 0.48 0.57 0.53 0.66]\n",
      "   [0.63 0.64 0.19 0.37 0.46 0.54 0.50 0.64]]\n",
      "\n",
      "  [[0.51 0.60 0.60 0.56 0.55 0.52 0.76 0.63]\n",
      "   [0.51 0.61 0.62 0.57 0.54 0.52 0.77 0.64]\n",
      "   [0.52 0.60 0.56 0.54 0.53 0.48 0.76 0.59]\n",
      "   [0.50 0.61 0.58 0.54 0.55 0.50 0.76 0.61]]]]\n",
      "diff:  [[[[0.23 0.18 0.05 -0.04 0.48 0.37 0.39 0.11]\n",
      "   [0.01 0.26 -0.05 -0.09 0.15 -0.00 0.01 -0.08]\n",
      "   [-0.04 -0.02 0.06 0.07 0.10 -0.02 -0.05 -0.02]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]\n",
      "\n",
      "  [[-0.44 0.28 0.16 -0.32 0.15 0.09 0.15 0.19]\n",
      "   [-0.31 0.13 0.05 -0.24 0.20 0.14 -0.04 0.14]\n",
      "   [-0.15 0.08 0.18 -0.02 0.08 0.16 0.03 0.18]\n",
      "   [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def scaled_dotproduct_attention_torch(q,k,v,seq_size):\n",
    "    \"\"\"torch版Attentio\n",
    "    参考　https://github.com/karpathy/nanoGPT/blob/master/model.py#L66\"\"\"\n",
    "    T = seq_size\n",
    "    # manual implementation of attention\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    if(IS_DEBUG): print(\"att: \", att)\n",
    "    # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))  # マスクは除外\n",
    "    att = F.softmax(att, dim=-1)\n",
    "    if(IS_DEBUG): print(\"softmax(att): \", att)\n",
    "    # att = self.attn_dropout(att)  # ドロップアウトは除外\n",
    "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    return y\n",
    "\n",
    "out_torch = scaled_dotproduct_attention_torch(Q.to_torch(), K.to_torch(), V.to_torch(), sequence_size)\n",
    "print(\"out: \", out_torch.numpy())\n",
    "print(\"diff: \", out.to_numpy()-out_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4f96-14ff-44f4-9aa5-d325e24fee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
